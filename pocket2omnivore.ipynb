{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "2IrD-ZoJQAPD"
   },
   "source": [
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/daviddavo/pocket2omnivore/HEAD?labpath=pocket2omnivore.ipynb)\n",
    "\n",
    "> This notebook is part of the [How to export your Pocket data and migrate to Omnivore](https://blog.ddavo.me/posts/tutorials/pocket-to-omnivore) tutorial\n",
    "\n",
    "# 1. Upload, parse, and store the Pocket File. \n",
    "\n",
    "First, let upload the `ril_export.html` file generated in https://getpocket.com/export\n",
    "\n",
    "The html has the following extructure:\n",
    "\n",
    "- `<h1>` Unread\n",
    "- `<ul>` with list items of `<a>`. The href is the link to the article, and the anchor text is the title. It also has a `tags` and `time_added` attributes.\n",
    "- `<h1>` Read\n",
    "- Another `<ul>` like the one above\n",
    "\n",
    "We will transform this into a dict of: \n",
    "- *read*: Boolean on wether the article has been read\n",
    "- *time_added*: The time the item was added\n",
    "- *tags*: An array of strings\n",
    "- *href*: The url\n",
    "- *title*: The title of the article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "id": "ez79reGjPokG",
    "outputId": "13422d0c-3dff-40ff-b0aa-c43fad089867",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ipywidgets import FileUpload\n",
    "from IPython.display import display\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "upload = FileUpload(accept='.html', multiple=False)\n",
    "\n",
    "def save_file():\n",
    "    for v in upload.value:\n",
    "        content = v['content']\n",
    "        with open(v['name'], 'wb') as f:\n",
    "            f.write(bytes(content))\n",
    "\n",
    "upload.observe(save_file, names='value')\n",
    "\n",
    "display(upload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert Path(\"ril_export.html\").exists(), \"Upload the file before continue running\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wEUX9qenQIBM",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "OMNIVORE_API_URL = \"https://api-prod.omnivore.app/api/graphql\"\n",
    "# The API key will have the following format \"00000000-0000-0000-0000-000000000000\"\n",
    "OMNIVORE_API_KEY = os.environ.get('OMNIVORE_API_KEY')\n",
    "SCHEMA_URL = \"https://raw.githubusercontent.com/omnivore-app/omnivore/c9fcbe72ddc6f40dd06e7073b8ffe3c1e71bd650/packages/api/src/generated/schema.graphql\"\n",
    "REQUESTS_SLEEP_TIME = 60 # Number of seconds\n",
    "\n",
    "if not OMNIVORE_API_KEY:\n",
    "    OMNIVORE_API_KEY=input('Enter your omnivore API key (should have a format similar to 00000000-0000-0000-0000-000000000000)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ka7QucTeQUFs",
    "outputId": "c85f5ad9-a75e-4a47-fa7b-e26c7c1313f5"
   },
   "outputs": [],
   "source": [
    "with open('ril_export.html', 'r') as f:\n",
    "    soup = BeautifulSoup(f, 'html.parser')\n",
    "\n",
    "soup.title"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the articles and tags from the HTML doc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IEYZUjJFRshV"
   },
   "outputs": [],
   "source": [
    "def process_list(h1):\n",
    "    ul = h1.find_next_sibling('ul')\n",
    "    print(len(ul), h1.text, 'articles')\n",
    "    read = h1.text != 'Unread'\n",
    "\n",
    "    items = []\n",
    "    for a in ul.findAll('a', href=True):\n",
    "        items.append({\n",
    "            'read': read,\n",
    "            'time_added': datetime.fromtimestamp(int(a['time_added'])),\n",
    "            'href': a['href'],\n",
    "            'tags': a['tags'].split(','),\n",
    "            'title': a.text,\n",
    "        })\n",
    "\n",
    "    return items\n",
    "\n",
    "articles = [item for sublist in [process_list(h1) for h1 in soup.findAll('h1')] for item in sublist]\n",
    "labels = set([item for sublist in [article['tags'] for article in articles if article['tags'][0] != ''] for item in sublist])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Store the articles and tags in a SQLLite Database\n",
    "\n",
    "We want to be able to track our process, as the API for Omnivore has rate limiting, and takes a while to upload the files. For this we will use a SQL Database.\n",
    "\n",
    "We will store the Articles in the format: \n",
    "- *read*: Boolean on wether the article has been read\n",
    "- *time_added*: The time the item was added\n",
    "- *tags*: An array of strings\n",
    "- *href*: The url\n",
    "- *id*: nullable field that stores the omnivore file.\n",
    "\n",
    "and the tags with:\n",
    "- *name*: Name of the tag \n",
    "- *id*: if the tag has been stored. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect('omnivore.db')\n",
    "\n",
    "# Create a cursor object to execute SQL commands\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Create the Article Table \n",
    "cursor.execute('''CREATE TABLE IF NOT EXISTS articles (\n",
    "                    id TEXT nullable,\n",
    "                    read BOOLEAN,\n",
    "                    time_added TEXT,\n",
    "                    tags TEXT, \n",
    "                    href TEXT PRIMARY KEY \n",
    "                )''')\n",
    "\n",
    "cursor.execute('''CREATE TABLE IF NOT EXISTS tags (\n",
    "                    id TEXT nullable,\n",
    "                    name TEXT PRIMARY KEY\n",
    "                )''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "insert_tag_sql = f\"\"\"INSERT OR IGNORE into tags (name) values (?)\"\"\"\n",
    "cursor.executemany(insert_tag_sql, [(label,) for label in labels])\n",
    "\n",
    "###\n",
    "#             'read': read,\n",
    "#             'time_added': datetime.fromtimestamp(int(a['time_added'])),\n",
    "#             'href': a['href'],\n",
    "#             'tags': a['tags'].split(','),\n",
    "#             'title': a.text,\n",
    "insert_article_sql = f\"INSERT OR IGNORE into articles (read, time_added, href, tags) values (?,?,?,?)\"\n",
    "article_values = [(article['read'], article['time_added'].isoformat(), article['href'], json.dumps(article['tags'])) for article in articles]\n",
    "cursor.executemany(insert_article_sql, article_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ei9PTNIzawUo",
    "outputId": "e0adcbcd-278e-4d3f-d5c2-c5db0c9c3de7"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "with requests.get(SCHEMA_URL) as r:\n",
    "    r.raise_for_status()\n",
    "    schema = r.text\n",
    "\n",
    "    assert schema is not None\n",
    "\n",
    "print(schema[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vIQr1xQ9fq-W"
   },
   "outputs": [],
   "source": [
    "from gql import gql, Client\n",
    "from gql.transport.aiohttp import AIOHTTPTransport\n",
    "\n",
    "def create_client():\n",
    "    transport = AIOHTTPTransport(\n",
    "       url=OMNIVORE_API_URL,\n",
    "        headers = {\n",
    "            'authorization': OMNIVORE_API_KEY,\n",
    "        }\n",
    "    )\n",
    "    return Client(transport=transport, schema=schema, fetch_schema_from_transport=False, execute_timeout=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sJNbK-UaBr0-",
    "outputId": "555ac6ee-8e64-431f-928c-475f8a1a8d02"
   },
   "outputs": [],
   "source": [
    "# Doing a \"test query\" to check if everything is correct\n",
    "\n",
    "async with create_client() as session: \n",
    "    r = session.execute(gql(\"\"\"\n",
    "    query Viewer {\n",
    "        me {\n",
    "            id\n",
    "            name\n",
    "            profile {\n",
    "                username\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \"\"\"))\n",
    "\n",
    "    result = await r\n",
    "    USERNAME = result['me']['profile']['username']\n",
    "\n",
    "    print(f\"Hello {result['me']['name']} ({USERNAME})!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_to_tag(row):  \n",
    "   return { \n",
    "      \"name\": row[1],\n",
    "      \"id\": row[0]\n",
    "   }\n",
    "\n",
    "all_tags = [row_to_tag(row) for row in cursor.execute('select * from tags').fetchall()]\n",
    "unsaved_tags = list(filter(lambda row: row['id'] is None, all_tags))\n",
    "presaved_tags = list(filter(lambda row: row['id'] is not None, all_tags))\n",
    "\n",
    "#Then remove all the tags from the ones we created before\n",
    "async def saveTags(tagName): \n",
    "    async with create_client() as client: \n",
    "      mutation = f\"\"\"\n",
    "      mutation {{\n",
    "        createLabel(input: {{color: \"#000\", name: \"{tagName}\" }}) {{\n",
    "          ... on CreateLabelSuccess {{\n",
    "            label {{\n",
    "              id\n",
    "              name\n",
    "              color\n",
    "              description\n",
    "              createdAt\n",
    "            }}\n",
    "          }}\n",
    "          ... on CreateLabelError {{\n",
    "            errorCodes\n",
    "          }}\n",
    "        }}\n",
    "      }}\n",
    "      \"\"\"\n",
    "      r = await client.execute(gql(mutation))\n",
    "      print(r)\n",
    "      return r['createLabel']['label']['id']\n",
    "\n",
    "tagIds = {}\n",
    "for tagValue in unsaved_tags: \n",
    "    try:\n",
    "        tag = tagValue['name']\n",
    "        id = await saveTags(tag)\n",
    "        tagIds[tag] = id\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", e)\n",
    "\n",
    "\n",
    "query = \"UPDATE tags SET id = ? where name = ?\"\n",
    "cursor.executemany(query, [(value, key) for key, value in tagIds.items()])\n",
    "\n",
    "# Add all the ones already saved.\n",
    "tagIds.update({f\"{dictionary['name']}\": dictionary[\"id\"] for dictionary in presaved_tags})\n",
    "tagIds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kxdnsdP4hYA8",
    "outputId": "16a47c1a-dee1-4fdf-d6d3-74ee2add63b1"
   },
   "outputs": [],
   "source": [
    "import backoff\n",
    "import asyncio\n",
    "\n",
    "createArticle = gql(\"\"\"\n",
    "  mutation CreateArticleSavingRequest($url: String!) {\n",
    "    createArticleSavingRequest(input: {url: $url}) {\n",
    "      ... on CreateArticleSavingRequestSuccess {\n",
    "        articleSavingRequest {\n",
    "          id\n",
    "          status\n",
    "          slug\n",
    "          createdAt\n",
    "          updatedAt\n",
    "          url\n",
    "          errorCode\n",
    "        }\n",
    "      }\n",
    "      ... on CreateArticleSavingRequestError {\n",
    "        errorCodes\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "\"\"\")\n",
    "\n",
    "setLabels = gql(\"\"\"\n",
    "mutation SetLabel($articleId: ID!, $labelIds: [ID!]!) { \n",
    "    setLabels(input: {pageId: $articleId, labelIds: $labelIds}) {\n",
    "        ...on SetLabelsSuccess { \n",
    "            labels { \n",
    "                id\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\"\"\")\n",
    "\n",
    "updatePageSavedDate =  gql(\"\"\"\n",
    "mutation UpdatePageDate($id: ID!, $date: Date!) {\n",
    "    updatePage(input: {pageId: $id, savedAt: $date, publishedAt: $date}) {\n",
    "        ... on UpdatePageSuccess {\n",
    "            updatedPage {\n",
    "                id\n",
    "                savedAt\n",
    "                publishedAt\n",
    "                title\n",
    "            }\n",
    "        }\n",
    "        ...on UpdatePageError {\n",
    "            errorCodes\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\"\"\")\n",
    "\n",
    "archivePage = gql(\"\"\"\n",
    "mutation ArchivePage($id: ID!) {\n",
    "    setLinkArchived (input: {linkId: $id, archived: true}) {\n",
    "        ... on ArchiveLinkSuccess {\n",
    "            linkId\n",
    "            message\n",
    "        }\n",
    "        ... on ArchiveLinkError {\n",
    "            message\n",
    "            errorCodes\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\"\"\")\n",
    "\n",
    "createTag = gql(\"\"\"\n",
    "mutation CreateLabel($nam: String!, $col: String, $desc: String) {\n",
    "  createLabel(input: {name: $nam, color: $col, description: $desc}) {\n",
    "    ... on CreateLabelSuccess {\n",
    "      label {\n",
    "        id\n",
    "        name\n",
    "        color\n",
    "        description\n",
    "        createdAt\n",
    "      }\n",
    "    }\n",
    "    ... on CreateLabelError {\n",
    "      errorCodes\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\"\"\")\n",
    "                \n",
    "@backoff.on_predicate(\n",
    "    backoff.runtime,\n",
    "    predicate=lambda r: isinstance(r, AIOHTTPTransport),\n",
    "    value=lambda r: int(r.response_headers[\"RateLimit-Reset\"]) + 1,\n",
    "    jitter=None,\n",
    ")\n",
    "async def archiveArticle(articleId): \n",
    "  async with create_client() as client: \n",
    "    try: \n",
    "      res = await client.execute(archivePage, { 'id': articleId })\n",
    "      return res\n",
    "    except Exception as e:\n",
    "          if (hasattr(e, 'code') and e.code == 429): \n",
    "            return session.transport\n",
    "          print(e)\n",
    "\n",
    "@backoff.on_predicate(\n",
    "    backoff.runtime,\n",
    "    predicate=lambda r: isinstance(r, AIOHTTPTransport),\n",
    "    value=lambda r: int(r.response_headers[\"RateLimit-Reset\"]) + 1,\n",
    "    jitter=None,\n",
    ")\n",
    "async def saveLabels(articleId, labels): \n",
    "    async with create_client() as client: \n",
    "      try:\n",
    "        client.execute(setLabels, {'articleId': articleId, 'labelIds': labels})\n",
    "      except Exception as e:\n",
    "          if (hasattr(e, 'code') and e.code == 429): \n",
    "            return session.transport\n",
    "          print(e)\n",
    "\n",
    "@backoff.on_predicate(\n",
    "    backoff.runtime,\n",
    "    predicate=lambda r: isinstance(r, AIOHTTPTransport),\n",
    "    value=lambda r: int(r.response_headers[\"RateLimit-Reset\"]) + 1,\n",
    "    jitter=None,\n",
    ")\n",
    "async def saveArticle(article):\n",
    "    async with create_client() as client: \n",
    "      try: \n",
    "        url = article['href']\n",
    "        tags = article['tags']\n",
    "        # First createArticleSavingRequest\n",
    "        r = await client.execute(createArticle, variable_values={'url': url})\n",
    "        print(r)\n",
    "        rid = r['createArticleSavingRequest']['articleSavingRequest']['id']\n",
    "        \n",
    "        if len(tags) != 0: \n",
    "            await saveLabels(rid, tags)\n",
    "                            \n",
    "        # Return the article with the id of the saved document\n",
    "        return {**article, 'id': rid }\n",
    "      except Exception as e:\n",
    "          if (hasattr(e, 'code') and e.code == 429): \n",
    "            return session.transport\n",
    "          # I don't know why this happens and I will figure it out later.\n",
    "          print(e)\n",
    "\n",
    "@backoff.on_predicate(\n",
    "    backoff.runtime,\n",
    "    predicate=lambda r: isinstance(r, AIOHTTPTransport),\n",
    "    value=lambda r: int(r.response_headers[\"RateLimit-Reset\"]) + 1,\n",
    "    jitter=None,\n",
    ")\n",
    "async def updateArticleTimeAfterProcessing(articleId, date = None):\n",
    "    async with create_client() as client: \n",
    "        try: \n",
    "            if date is not None: \n",
    "              # Wait a bit, it seems there's a race condition.\n",
    "              res = await client.execute(updatePageSavedDate, {\n",
    "                  'id': articleId,\n",
    "                  'date': date,\n",
    "              })\n",
    "              return res\n",
    "\n",
    "        except Exception as e:\n",
    "          if (hasattr(e, 'code') and e.code == 429): \n",
    "            return session.transport\n",
    "          \n",
    "          # I don't know why this happens and I will figure it out later.\n",
    "          print(e)\n",
    "\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio \n",
    "\n",
    "BATCH_UPDATE_SIZE = 100 # Bad name, we save to the DB after 100 so that we have a stop point.\n",
    "PARALLEL_API_CALL_SIZE = 10;\n",
    "\n",
    "def row_to_article(row):  \n",
    "   return { \n",
    "      \"id\": row[0],\n",
    "      \"read\": bool(row[1]),\n",
    "      \"time_added\": row[2],\n",
    "      \"tags\": json.loads(row[3]),\n",
    "      \"href\": row[4]\n",
    "   }\n",
    "\n",
    "all_articles = [row_to_article(row) for row in cursor.execute('select * from articles').fetchall()]\n",
    "unsaved_articles = list(filter(lambda article: article['id'] is None, all_articles)) \n",
    "saved_articles = list(filter(lambda article: article['id'] is not None, all_articles)) \n",
    "\n",
    "saved_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articleBatches = [unsaved_articles[i:i+BATCH_UPDATE_SIZE] for i in range(0, len(unsaved_articles), BATCH_UPDATE_SIZE)]\n",
    "savedArticles = [] + saved_articles\n",
    "for batch in articleBatches: \n",
    "   parrallel_requests = [batch[i:i+PARALLEL_API_CALL_SIZE] for i in range(0, len(batch), PARALLEL_API_CALL_SIZE)]\n",
    "   batchSaved = []\n",
    "   iteration = 0;\n",
    "   # From the 10, save them. \n",
    "   for parallel_call_batch in parrallel_requests: \n",
    "      futures = []\n",
    "      # Concurrently execute everything in the batch \n",
    "      for article in parallel_call_batch: \n",
    "         futures.append(asyncio.ensure_future(saveArticle(article)))\n",
    "\n",
    "      print(futures)\n",
    "      iteration = iteration + 1\n",
    "      print(iteration)\n",
    "\n",
    "      completedFutures = [] \n",
    "      completedFutures = await asyncio.gather(*futures);\n",
    "         \n",
    "      # Then wait for the respsonses before completing the next batch\n",
    "      for savedArticle in completedFutures:\n",
    "         batchSaved.append(savedArticle)\n",
    "         savedArticles.append(savedArticle)\n",
    "\n",
    "    # Archive if it is needed. \n",
    "   archive_futures = []\n",
    "   batched_saved_articles = [batchSaved[i:i+PARALLEL_API_CALL_SIZE] for i in range(0, len(batchSaved), PARALLEL_API_CALL_SIZE)]\n",
    "   for archive_batch in batched_saved_articles: \n",
    "      for article in archive_batch:\n",
    "         archive_futures.append(asyncio.ensure_future(updateArticleTimeAfterProcessing(article['id'], article['time_added'])))\n",
    "         if (article['read'] == True): \n",
    "            archive_futures.append(asyncio.ensure_future(archiveArticle(article['id'])))\n",
    "            \n",
    "      results = await asyncio.gather(*archive_futures);\n",
    "      print(len(results))\n",
    "   # for article in batchSaved: \n",
    "   #    archive_futures.append(asyncio.ensure_future(updateArticleTimeAfterProcessing(article['id'], article['time_added'])))\n",
    "   #    await asyncio.gather(*archive_futures);\n",
    "\n",
    "   \n",
    "   query = \"UPDATE articles SET id = ? where href = ?\"\n",
    "   cursor.executemany(query, [(saved_article['id'], saved_article['href']) for saved_article in batchSaved])\n",
    "   "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
